---
layout: post
title: "[오태경의 머신러닝] 5일 단기 5일차 (1)"
excerpt: "딥러닝 - 심층 신경망"
categories:
- Excel
tags:
- Excel
- 오태경의 머신러닝
- 5일 단기
last_modified_at: 2021-07-31T15:25:00+09:00
comments : true
---
<hr>

<h2>- 목차</h2>
<div style="align-items: center;">
    <img src="/assets/post-image/Excel-5일-단기-5/슬라이드3.PNG">
</div>
<br>

<h2>- Intro</h2>
<div style="align-items: center;">
    <img src="/assets/post-image/Excel-5일-단기-5/슬라이드1.PNG">
</div>
<div style="align-items: center;">
    <img src="/assets/post-image/Excel-5일-단기-5/슬라이드2.PNG">
</div>
<div style="align-items: center;">
    <img src="/assets/post-image/Excel-5일-단기-5/슬라이드4.PNG">
</div>

> 마지막 교육일을 기념으로 인트로와 전체적인 ppt 디자인을 좀 더 무거운 느낌으로 바꿔보았다.

<br>
<h2>- 심층 신경망</h2>
<div style="align-items: center;">
    <img src="/assets/post-image/Excel-5일-단기-5/슬라이드5.PNG">
</div>

<br>
<h2>- 활성화함수로써의 ReLU</h2>
<div style="align-items: center;">
    <img src="/assets/post-image/Excel-5일-단기-5/슬라이드6.PNG">
</div>

> 엑셀로 구현한 신경망에 한해서, 매우 작은 수의 소수점 자릿수가 증가할 때 0으로 계산되어 특정 노드가 꺼져버려서 몇 개의 노드에서 가중치가 폭주하는 현상이 발생하는데, 층마다 노드의 개수를 같게 해줌으로써 해결된다는 사실을 경험적으로 터득하였다. 따라서 두 번째 설명인 "각 층의 노드의 개수를 똑같이 맞춰 줌"은 엑셀로 구현한 크기가 작은 신경망에 해당하는 설명이기 때문에 충분히 큰 신경망을 만들거나 다른 언어를 사용한다면 해당 설명은 무시해도 좋다. (혹은 Leaky ReLU를 써도 해결된다.)

<br>
<h2>- DNN의 경사하강법</h2>
<div style="align-items: center;">
    <img src="/assets/post-image/Excel-5일-단기-5/슬라이드7.PNG">
</div>
<div style="align-items: center;">
    <img src="/assets/post-image/Excel-5일-단기-5/슬라이드8.PNG">
</div>

> 각 층마다 각 노드의 편미분을 하나하나 계산하여 코딩하는 것은 정말 어려울 것이다. 따라서 층이 깊은 신경망에서는 "오차역전파"라는 개념을 적용하여 직접적인 편미분 없이 가중치를 업데이트 하는 방법을 사용한다.

<br>